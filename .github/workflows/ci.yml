name: CI

on:
  push:
    branches:
    - main
    tags:
    - '*'
  pull_request:
env:
  LATEST_PY_VERSION: '3.13'

jobs:
  tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version:
          - '3.9'
          - '3.10'
          - '3.11'
          - '3.12'
          - '3.13'
          # - '3.14.0-alpha.2' wait for pyproj and rasterio wheels to support 3.14

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install .["test"]

      - name: run pre-commit
        if: ${{ matrix.python-version == env.LATEST_PY_VERSION }}
        run: |
          python -m pip install pre-commit
          pre-commit run --all-files

      - name: Run tests
        run: python -m pytest --cov morecantile --cov-report term-missing --cov-report xml

      - name: Upload Results
        if: ${{ matrix.python-version == env.LATEST_PY_VERSION }}
        uses: codecov/codecov-action@v1
        with:
          file: ./coverage.xml
          flags: unittests
          name: Python${{ matrix.python-version }}
          fail_ci_if_error: false

  benchmark:
    needs: [tests]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[benchmark]"

      - name: Run Benchmark
        run: |
          python -m pytest tests/benchmarks.py --benchmark-only --benchmark-columns 'min, max, mean, median' --benchmark-sort 'min' --benchmark-json output.json

      - name: Store and Compare benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: morecantile Benchmarks
          tool: 'pytest'
          output-file-path: output.json
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          # GitHub API token to make a commit comment
          github-token: ${{ secrets.GITHUB_TOKEN }}
          gh-pages-branch: 'gh-benchmarks'
          # Make a commit on `gh-pages` only if main
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          benchmark-data-dir-path: dev/benchmarks

  publish:
    needs: [tests]
    runs-on: ubuntu-latest
    if: startsWith(github.event.ref, 'refs/tags') || github.event_name == 'release'
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.LATEST_PY_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install flit
          python -m pip install .

      - name: Set tag version
        id: tag
        run: |
          echo "version=${GITHUB_REF#refs/*/}"
          echo "version=${GITHUB_REF#refs/*/}" >> $GITHUB_OUTPUT

      - name: Set module version
        id: module
        run: |
          echo version=$(python -c'import morecantile; print(morecantile.__version__)') >> $GITHUB_OUTPUT

      - name: Build and publish
        if: ${{ steps.tag.outputs.version }} == ${{ steps.module.outputs.version}}
        env:
          FLIT_USERNAME: ${{ secrets.PYPI_USERNAME }}
          FLIT_PASSWORD: ${{ secrets.PYPI_PASSWORD }}
        run: flit publish
